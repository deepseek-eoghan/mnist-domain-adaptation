# @package _global_

# to execute this experiment run:
# python run.py experiment=mnist_adapt_example.yaml

defaults:
  - override /mode: exp.yaml
  - override /trainer: default.yaml
  - override /model: mnist_adapt_model.yaml
  - override /datamodule: mnist_adapt_datamodule.yaml
  - override /callbacks: wandb.yaml #wandb.yaml
  - override /logger: wandb.yaml # wandb.yaml

# all parameters below will be merged with parameters from default configurations set above
# this allows you to overwrite only specified parameters

# name of the run determines folder name in logs
# it's also accessed by loggers
name: "debug_run_3"

seed: 100101101112115101101107046097105

# logger:
#   wandb:
#     log_model: False
#     offline: True

trainer:
  min_epochs: 1
  max_epochs: 25
  gpus: 1
  limit_train_batches: 0.025
  limit_val_batches: 0.025
  limit_test_batches: 0.025

lr_finder: null
batch_finder: null
test_after_training: False # Remain false until action on https://github.com/KevinMusgrave/pytorch-adapt/issues/58