# @package _global_

# to execute this experiment run:
# python run.py experiment=mnist_adapt_example.yaml

defaults:
  - override /mode: exp.yaml
  - override /trainer: default.yaml
  - override /model: mnist.yaml
  - override /datamodule: mnist_adapt_datamodule.yaml
  - override /callbacks: wandb.yaml #wandb.yaml
  - override /logger: wandb.yaml # wandb.yaml

# all parameters below will be merged with parameters from default configurations set above
# this allows you to overwrite only specified parameters

# name of the run determines folder name in logs
# it's also accessed by loggers
name: "debug_run_mnist"

logger:
  wandb:
    log_model: False
    offline: True

seed: 100101101112115101101107046097105

trainer:
  min_epochs: 1
  max_epochs: 25
  gpus: 1
  limit_train_batches: 0.025
  limit_val_batches: 0.025
  limit_test_batches: 0.025

adapt: False

test_after_training: False # Remain false until action on https://github.com/KevinMusgrave/pytorch-adapt/issues/58